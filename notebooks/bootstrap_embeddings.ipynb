{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khsd3rWefzQS",
        "outputId": "65d375b3-bfdd-4a4c-ab5c-2816a174075f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq --progress-bar off  pandas transformers accelerate"
      ],
      "metadata": {
        "id": "6X-NasD5gFAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573e6efb-dfd4-4b4a-c077-8dacfb532db0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from time import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "from torch import Tensor\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "TFONFuvdrxGP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumes GPU\n",
        "\n",
        "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "def embed(tokenizer, model, texts, batch_size=1):\n",
        "    # Sentence transformers for E5\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        if i % 100 == 0:\n",
        "          print(\"batch:\", i)\n",
        "        batch_dict = tokenizer(\n",
        "            texts[i : i + batch_size],\n",
        "            max_length=512,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "        outputs = model(**batch_dict)\n",
        "\n",
        "        vectors = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
        "        vectors = F.normalize(vectors, p=2, dim=1)\n",
        "        # print(type(vectors)) -> Tensor\n",
        "        # print(vectors.shape) -> (n_doc X size_embedding)\n",
        "        embeddings.append(vectors.detach().cpu().numpy())\n",
        "        #torch.cuda.empty_cache()\n",
        "\n",
        "    #return torch.cat(embeddings) # burn memory\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "now = time()\n",
        "\n",
        "device_map = \"cuda:0\"\n",
        "\n",
        "# sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"e5-multilingual\", device = \"cuda\")\n",
        "model_name = \"intfloat/multilingual-e5-base\"\n",
        "#model_name = \"intfloat/multilingual-e5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, device_map=device_map)  # -> SentenceTransformer\n",
        "\n",
        "loading_time = time()\n",
        "print(\"Loading time: %.3f\" % (loading_time - now))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQdE_LbwmADz",
        "outputId": "e9d7e19e-cde5-4792-fd05-c6f126ad7af7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading time: 2.683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# EXPERIENCES\n",
        "#\n",
        "\n",
        "with open(\"_data/export-expa-c-riences.json\") as f:\n",
        "    documents = json.load(f)\n",
        "\n",
        "def add_space_after_punctuation(text):\n",
        "    return re.sub(r'([.,;:!?])([^\\s\\d])', r'\\1 \\2', text)\n",
        "\n",
        "for d in documents:\n",
        "  descr = d[\"description\"]\n",
        "  d[\"description\"] = re.sub(r'([.,;!?])([^\\s])', r'\\1 \\2', descr)\n"
      ],
      "metadata": {
        "id": "ze91oh-1nOL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = time()\n",
        "texts = [x[\"description\"] for x in documents]\n",
        "embeddings = embed(tokenizer, model, texts, batch_size=4)\n",
        "print(\"Inference time: %.3f\" % (time() - now))\n",
        "\n",
        "np.save('embeddings_e5_experiences.npy', embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGPKg9om1Mn",
        "outputId": "4caaa283-b620-4dc3-fdaf-7c1e9813f052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time: 633.767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fINdwhWvpANY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUcO8tKDCmZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# CHUNKS\n",
        "#\n",
        "\n",
        "with open(\"_data/xmlfiles_as_chunks.json\") as f:\n",
        "    documents = json.load(f)\n",
        "\n",
        "for doc in documents:\n",
        "    if \"context\" in doc:\n",
        "        doc[\"context\"] = \" > \".join(doc[\"context\"])\n"
      ],
      "metadata": {
        "id": "yOJGdjv3Cl98"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = time()\n",
        "texts = [\" \".join([x[\"title\"], x[\"introduction\"], x[\"text\"], x.get(\"context\", \"\")]) for x in documents]\n",
        "embeddings = embed(tokenizer, model, texts, batch_size=4)\n",
        "print(\"Inference time: %.3f\" % (time() - now))\n",
        "\n",
        "np.save('embeddings_e5_chunks.npy', embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QpZ0Xx-DGtL",
        "outputId": "fd65c18f-d802-4917-b5f5-1fe3011f463f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0\n",
            "batch: 100\n",
            "batch: 200\n",
            "batch: 300\n",
            "batch: 400\n",
            "batch: 500\n",
            "batch: 600\n",
            "batch: 700\n",
            "batch: 800\n",
            "batch: 900\n",
            "batch: 1000\n",
            "batch: 1100\n",
            "batch: 1200\n",
            "batch: 1300\n",
            "batch: 1400\n",
            "batch: 1500\n",
            "batch: 1600\n",
            "batch: 1700\n",
            "batch: 1800\n",
            "batch: 1900\n",
            "batch: 2000\n",
            "batch: 2100\n",
            "batch: 2200\n",
            "batch: 2300\n",
            "batch: 2400\n",
            "batch: 2500\n",
            "batch: 2600\n",
            "batch: 2700\n",
            "batch: 2800\n",
            "batch: 2900\n",
            "batch: 3000\n",
            "batch: 3100\n",
            "batch: 3200\n",
            "batch: 3300\n",
            "batch: 3400\n",
            "batch: 3500\n",
            "batch: 3600\n",
            "batch: 3700\n",
            "batch: 3800\n",
            "batch: 3900\n",
            "batch: 4000\n",
            "batch: 4100\n",
            "batch: 4200\n",
            "batch: 4300\n",
            "batch: 4400\n",
            "batch: 4500\n",
            "batch: 4600\n",
            "batch: 4700\n",
            "batch: 4800\n",
            "batch: 4900\n",
            "batch: 5000\n",
            "batch: 5100\n",
            "batch: 5200\n",
            "batch: 5300\n",
            "batch: 5400\n",
            "batch: 5500\n",
            "batch: 5600\n",
            "batch: 5700\n",
            "batch: 5800\n",
            "batch: 5900\n",
            "batch: 6000\n",
            "batch: 6100\n",
            "batch: 6200\n",
            "batch: 6300\n",
            "batch: 6400\n",
            "batch: 6500\n",
            "batch: 6600\n",
            "batch: 6700\n",
            "batch: 6800\n",
            "batch: 6900\n",
            "batch: 7000\n",
            "batch: 7100\n",
            "batch: 7200\n",
            "batch: 7300\n",
            "batch: 7400\n",
            "batch: 7500\n",
            "batch: 7600\n",
            "batch: 7700\n",
            "batch: 7800\n",
            "batch: 7900\n",
            "batch: 8000\n",
            "batch: 8100\n",
            "batch: 8200\n",
            "batch: 8300\n",
            "batch: 8400\n",
            "batch: 8500\n",
            "batch: 8600\n",
            "batch: 8700\n",
            "batch: 8800\n",
            "batch: 8900\n",
            "batch: 9000\n",
            "batch: 9100\n",
            "batch: 9200\n",
            "batch: 9300\n",
            "batch: 9400\n",
            "batch: 9500\n",
            "batch: 9600\n",
            "batch: 9700\n",
            "batch: 9800\n",
            "batch: 9900\n",
            "batch: 10000\n",
            "batch: 10100\n",
            "batch: 10200\n",
            "batch: 10300\n",
            "batch: 10400\n",
            "batch: 10500\n",
            "batch: 10600\n",
            "batch: 10700\n",
            "batch: 10800\n",
            "batch: 10900\n",
            "batch: 11000\n",
            "batch: 11100\n",
            "batch: 11200\n",
            "batch: 11300\n",
            "batch: 11400\n",
            "batch: 11500\n",
            "batch: 11600\n",
            "batch: 11700\n",
            "batch: 11800\n",
            "batch: 11900\n",
            "batch: 12000\n",
            "batch: 12100\n",
            "batch: 12200\n",
            "batch: 12300\n",
            "batch: 12400\n",
            "batch: 12500\n",
            "batch: 12600\n",
            "batch: 12700\n",
            "batch: 12800\n",
            "batch: 12900\n",
            "batch: 13000\n",
            "batch: 13100\n",
            "batch: 13200\n",
            "batch: 13300\n",
            "batch: 13400\n",
            "batch: 13500\n",
            "batch: 13600\n",
            "batch: 13700\n",
            "batch: 13800\n",
            "batch: 13900\n",
            "batch: 14000\n",
            "batch: 14100\n",
            "batch: 14200\n",
            "batch: 14300\n",
            "batch: 14400\n",
            "batch: 14500\n",
            "batch: 14600\n",
            "batch: 14700\n",
            "batch: 14800\n",
            "batch: 14900\n",
            "batch: 15000\n",
            "batch: 15100\n",
            "batch: 15200\n",
            "batch: 15300\n",
            "batch: 15400\n",
            "batch: 15500\n",
            "batch: 15600\n",
            "batch: 15700\n",
            "batch: 15800\n",
            "batch: 15900\n",
            "batch: 16000\n",
            "batch: 16100\n",
            "batch: 16200\n",
            "batch: 16300\n",
            "batch: 16400\n",
            "batch: 16500\n",
            "batch: 16600\n",
            "batch: 16700\n",
            "batch: 16800\n",
            "batch: 16900\n",
            "batch: 17000\n",
            "batch: 17100\n",
            "batch: 17200\n",
            "batch: 17300\n",
            "batch: 17400\n",
            "batch: 17500\n",
            "batch: 17600\n",
            "batch: 17700\n",
            "batch: 17800\n",
            "batch: 17900\n",
            "batch: 18000\n",
            "batch: 18100\n",
            "batch: 18200\n",
            "batch: 18300\n",
            "batch: 18400\n",
            "batch: 18500\n",
            "batch: 18600\n",
            "batch: 18700\n",
            "batch: 18800\n",
            "batch: 18900\n",
            "batch: 19000\n",
            "batch: 19100\n",
            "batch: 19200\n",
            "batch: 19300\n",
            "batch: 19400\n",
            "batch: 19500\n",
            "batch: 19600\n",
            "batch: 19700\n",
            "batch: 19800\n",
            "batch: 19900\n",
            "batch: 20000\n",
            "batch: 20100\n",
            "batch: 20200\n",
            "batch: 20300\n",
            "batch: 20400\n",
            "batch: 20500\n",
            "batch: 20600\n",
            "batch: 20700\n",
            "batch: 20800\n",
            "batch: 20900\n",
            "batch: 21000\n",
            "batch: 21100\n",
            "batch: 21200\n",
            "batch: 21300\n",
            "batch: 21400\n",
            "batch: 21500\n",
            "batch: 21600\n",
            "batch: 21700\n",
            "batch: 21800\n",
            "batch: 21900\n",
            "batch: 22000\n",
            "batch: 22100\n",
            "batch: 22200\n",
            "batch: 22300\n",
            "batch: 22400\n",
            "batch: 22500\n",
            "batch: 22600\n",
            "batch: 22700\n",
            "batch: 22800\n",
            "batch: 22900\n",
            "batch: 23000\n",
            "batch: 23100\n",
            "batch: 23200\n",
            "batch: 23300\n",
            "batch: 23400\n",
            "batch: 23500\n",
            "batch: 23600\n",
            "batch: 23700\n",
            "batch: 23800\n",
            "batch: 23900\n",
            "batch: 24000\n",
            "batch: 24100\n",
            "batch: 24200\n",
            "batch: 24300\n",
            "batch: 24400\n",
            "batch: 24500\n",
            "batch: 24600\n",
            "batch: 24700\n",
            "batch: 24800\n",
            "batch: 24900\n",
            "batch: 25000\n",
            "batch: 25100\n",
            "batch: 25200\n",
            "batch: 25300\n",
            "batch: 25400\n",
            "batch: 25500\n",
            "batch: 25600\n",
            "batch: 25700\n",
            "batch: 25800\n",
            "batch: 25900\n",
            "batch: 26000\n",
            "batch: 26100\n",
            "batch: 26200\n",
            "batch: 26300\n",
            "batch: 26400\n",
            "batch: 26500\n",
            "batch: 26600\n",
            "Inference time: 764.052\n"
          ]
        }
      ]
    }
  ]
}