{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Khsd3rWefzQS",
    "outputId": "943358dd-539e-4ea4-c1a3-a80586df058c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6X-NasD5gFAr",
    "outputId": "239db336-9f02-4e0e-b10f-8ecc23be2e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq --progress-bar off  pandas transformers accelerate sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TFONFuvdrxGP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQdE_LbwmADz",
    "outputId": "d29251e6-9420-465b-bf88-6a1b3b31ced4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 3.079\n"
     ]
    }
   ],
   "source": [
    "# Assumes GPU\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "def embed(tokenizer, model, texts, batch_size=1): # 4 on colab GPU...\n",
    "    # Embedification for E5 (less optimized than SentenceTransformer)\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        if i % 100 == 0:\n",
    "          print(\"batch:\", i)\n",
    "\n",
    "        batch_dict = tokenizer(\n",
    "            texts[i : i + batch_size],\n",
    "            max_length=512,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        outputs = model(**batch_dict)\n",
    "\n",
    "        vectors = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
    "        vectors = F.normalize(vectors, p=2, dim=1)\n",
    "        # print(type(vectors)) -> Tensor\n",
    "        # print(vectors.shape) -> (n_doc X size_embedding)\n",
    "        embeddings.append(vectors.detach().cpu().numpy())\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "    #return torch.cat(embeddings) # burn memory\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def embed_(tok, model, texts, batch_size=1):\n",
    "  return model.encode(texts, normalize_embeddings=True, batch_size=32)\n",
    "\n",
    "\n",
    "now = time()\n",
    "\n",
    "device_map = \"cuda:0\"\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-base\"\n",
    "#model_name = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, device_map=device_map)  # -> SentenceTransformer\n",
    "#model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "\n",
    "loading_time = time()\n",
    "print(\"Loading time: %.3f\" % (loading_time - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ze91oh-1nOL6"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXPERIENCES\n",
    "#\n",
    "\n",
    "with open(\"_data/export-expa-c-riences.json\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "def add_space_after_punctuation(text):\n",
    "    return re.sub(r'([.,;:!?])([^\\s\\d])', r'\\1 \\2', text)\n",
    "\n",
    "for d in documents:\n",
    "  descr = d[\"description\"]\n",
    "  d[\"description\"] = add_space_after_punctuation(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrGPKg9om1Mn"
   },
   "outputs": [],
   "source": [
    "now = time()\n",
    "texts = [x[\"description\"] for x in documents]\n",
    "embeddings = embed(tokenizer, model, texts, batch_size=4)\n",
    "print(\"Inference time: %.3f\" % (time() - now))\n",
    "\n",
    "np.save('embeddings_e5_experiences.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fINdwhWvpANY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUcO8tKDCmZT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOJGdjv3Cl98"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# CHUNKS\n",
    "#\n",
    "\n",
    "with open(\"_data/xmlfiles_as_chunks.json\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "for doc in documents:\n",
    "    if \"context\" in doc:\n",
    "        doc[\"context\"] = \" > \".join(doc[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QpZ0Xx-DGtL",
    "outputId": "fd65c18f-d802-4917-b5f5-1fe3011f463f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "batch: 100\n",
      "batch: 200\n",
      "batch: 300\n",
      "batch: 400\n",
      "batch: 500\n",
      "batch: 600\n",
      "batch: 700\n",
      "batch: 800\n",
      "batch: 900\n",
      "batch: 1000\n",
      "batch: 1100\n",
      "batch: 1200\n",
      "batch: 1300\n",
      "batch: 1400\n",
      "batch: 1500\n",
      "batch: 1600\n",
      "batch: 1700\n",
      "batch: 1800\n",
      "batch: 1900\n",
      "batch: 2000\n",
      "batch: 2100\n",
      "batch: 2200\n",
      "batch: 2300\n",
      "batch: 2400\n",
      "batch: 2500\n",
      "batch: 2600\n",
      "batch: 2700\n",
      "batch: 2800\n",
      "batch: 2900\n",
      "batch: 3000\n",
      "batch: 3100\n",
      "batch: 3200\n",
      "batch: 3300\n",
      "batch: 3400\n",
      "batch: 3500\n",
      "batch: 3600\n",
      "batch: 3700\n",
      "batch: 3800\n",
      "batch: 3900\n",
      "batch: 4000\n",
      "batch: 4100\n",
      "batch: 4200\n",
      "batch: 4300\n",
      "batch: 4400\n",
      "batch: 4500\n",
      "batch: 4600\n",
      "batch: 4700\n",
      "batch: 4800\n",
      "batch: 4900\n",
      "batch: 5000\n",
      "batch: 5100\n",
      "batch: 5200\n",
      "batch: 5300\n",
      "batch: 5400\n",
      "batch: 5500\n",
      "batch: 5600\n",
      "batch: 5700\n",
      "batch: 5800\n",
      "batch: 5900\n",
      "batch: 6000\n",
      "batch: 6100\n",
      "batch: 6200\n",
      "batch: 6300\n",
      "batch: 6400\n",
      "batch: 6500\n",
      "batch: 6600\n",
      "batch: 6700\n",
      "batch: 6800\n",
      "batch: 6900\n",
      "batch: 7000\n",
      "batch: 7100\n",
      "batch: 7200\n",
      "batch: 7300\n",
      "batch: 7400\n",
      "batch: 7500\n",
      "batch: 7600\n",
      "batch: 7700\n",
      "batch: 7800\n",
      "batch: 7900\n",
      "batch: 8000\n",
      "batch: 8100\n",
      "batch: 8200\n",
      "batch: 8300\n",
      "batch: 8400\n",
      "batch: 8500\n",
      "batch: 8600\n",
      "batch: 8700\n",
      "batch: 8800\n",
      "batch: 8900\n",
      "batch: 9000\n",
      "batch: 9100\n",
      "batch: 9200\n",
      "batch: 9300\n",
      "batch: 9400\n",
      "batch: 9500\n",
      "batch: 9600\n",
      "batch: 9700\n",
      "batch: 9800\n",
      "batch: 9900\n",
      "batch: 10000\n",
      "batch: 10100\n",
      "batch: 10200\n",
      "batch: 10300\n",
      "batch: 10400\n",
      "batch: 10500\n",
      "batch: 10600\n",
      "batch: 10700\n",
      "batch: 10800\n",
      "batch: 10900\n",
      "batch: 11000\n",
      "batch: 11100\n",
      "batch: 11200\n",
      "batch: 11300\n",
      "batch: 11400\n",
      "batch: 11500\n",
      "batch: 11600\n",
      "batch: 11700\n",
      "batch: 11800\n",
      "batch: 11900\n",
      "batch: 12000\n",
      "batch: 12100\n",
      "batch: 12200\n",
      "batch: 12300\n",
      "batch: 12400\n",
      "batch: 12500\n",
      "batch: 12600\n",
      "batch: 12700\n",
      "batch: 12800\n",
      "batch: 12900\n",
      "batch: 13000\n",
      "batch: 13100\n",
      "batch: 13200\n",
      "batch: 13300\n",
      "batch: 13400\n",
      "batch: 13500\n",
      "batch: 13600\n",
      "batch: 13700\n",
      "batch: 13800\n",
      "batch: 13900\n",
      "batch: 14000\n",
      "batch: 14100\n",
      "batch: 14200\n",
      "batch: 14300\n",
      "batch: 14400\n",
      "batch: 14500\n",
      "batch: 14600\n",
      "batch: 14700\n",
      "batch: 14800\n",
      "batch: 14900\n",
      "batch: 15000\n",
      "batch: 15100\n",
      "batch: 15200\n",
      "batch: 15300\n",
      "batch: 15400\n",
      "batch: 15500\n",
      "batch: 15600\n",
      "batch: 15700\n",
      "batch: 15800\n",
      "batch: 15900\n",
      "batch: 16000\n",
      "batch: 16100\n",
      "batch: 16200\n",
      "batch: 16300\n",
      "batch: 16400\n",
      "batch: 16500\n",
      "batch: 16600\n",
      "batch: 16700\n",
      "batch: 16800\n",
      "batch: 16900\n",
      "batch: 17000\n",
      "batch: 17100\n",
      "batch: 17200\n",
      "batch: 17300\n",
      "batch: 17400\n",
      "batch: 17500\n",
      "batch: 17600\n",
      "batch: 17700\n",
      "batch: 17800\n",
      "batch: 17900\n",
      "batch: 18000\n",
      "batch: 18100\n",
      "batch: 18200\n",
      "batch: 18300\n",
      "batch: 18400\n",
      "batch: 18500\n",
      "batch: 18600\n",
      "batch: 18700\n",
      "batch: 18800\n",
      "batch: 18900\n",
      "batch: 19000\n",
      "batch: 19100\n",
      "batch: 19200\n",
      "batch: 19300\n",
      "batch: 19400\n",
      "batch: 19500\n",
      "batch: 19600\n",
      "batch: 19700\n",
      "batch: 19800\n",
      "batch: 19900\n",
      "batch: 20000\n",
      "batch: 20100\n",
      "batch: 20200\n",
      "batch: 20300\n",
      "batch: 20400\n",
      "batch: 20500\n",
      "batch: 20600\n",
      "batch: 20700\n",
      "batch: 20800\n",
      "batch: 20900\n",
      "batch: 21000\n",
      "batch: 21100\n",
      "batch: 21200\n",
      "batch: 21300\n",
      "batch: 21400\n",
      "batch: 21500\n",
      "batch: 21600\n",
      "batch: 21700\n",
      "batch: 21800\n",
      "batch: 21900\n",
      "batch: 22000\n",
      "batch: 22100\n",
      "batch: 22200\n",
      "batch: 22300\n",
      "batch: 22400\n",
      "batch: 22500\n",
      "batch: 22600\n",
      "batch: 22700\n",
      "batch: 22800\n",
      "batch: 22900\n",
      "batch: 23000\n",
      "batch: 23100\n",
      "batch: 23200\n",
      "batch: 23300\n",
      "batch: 23400\n",
      "batch: 23500\n",
      "batch: 23600\n",
      "batch: 23700\n",
      "batch: 23800\n",
      "batch: 23900\n",
      "batch: 24000\n",
      "batch: 24100\n",
      "batch: 24200\n",
      "batch: 24300\n",
      "batch: 24400\n",
      "batch: 24500\n",
      "batch: 24600\n",
      "batch: 24700\n",
      "batch: 24800\n",
      "batch: 24900\n",
      "batch: 25000\n",
      "batch: 25100\n",
      "batch: 25200\n",
      "batch: 25300\n",
      "batch: 25400\n",
      "batch: 25500\n",
      "batch: 25600\n",
      "batch: 25700\n",
      "batch: 25800\n",
      "batch: 25900\n",
      "batch: 26000\n",
      "batch: 26100\n",
      "batch: 26200\n",
      "batch: 26300\n",
      "batch: 26400\n",
      "batch: 26500\n",
      "batch: 26600\n",
      "Inference time: 764.052\n"
     ]
    }
   ],
   "source": [
    "now = time()\n",
    "texts = [\" \".join([x[\"title\"], x[\"introduction\"], x[\"text\"], x.get(\"context\", \"\")]) for x in documents]\n",
    "embeddings = embed(tokenizer, model, texts, batch_size=4)\n",
    "print(\"Inference time: %.3f\" % (time() - now))\n",
    "\n",
    "np.save('embeddings_e5_chunks.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TESMEtilTAbi"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TEST\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "au2nZr6fCswH",
    "outputId": "0ff415f3-b00d-49fa-d637-da50103647f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.10/dist-packages (1.5.4)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.58.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.58.0)\n",
      "Requirement already satisfied: httpx[http2]>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.23.5)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.7.0)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.10.12)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.26.16)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (4.24.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (67.7.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client) (0.18.0)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client) (3.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (4.5.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx[http2]>=0.14.0->qdrant-client) (3.7.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx[http2]>=0.14.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx[http2]>=0.14.0->qdrant-client) (1.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp1s8nyCC5Ws",
    "outputId": "ca76ffe1-fdce-466a-dc4c-bdb52655c36c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "client.recreate_collection(\n",
    "    collection_name=\"test\",\n",
    "    vectors_config=models.VectorParams(size=embeddings.shape[1], distance=models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=\"test\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=documents[i][\"id_experience\"],\n",
    "            vector=vector.tolist(),\n",
    "            # payload={\"color\": \"red\", \"rand_number\": idx % 10}\n",
    "        )\n",
    "        for i, vector in enumerate(embeddings)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0np0zdXrD5TF",
    "outputId": "b58ef8d9-1c52-4370-9715-136727b61d65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=3120859, version=0, score=0.88349164, payload={}, vector=None),\n",
       " ScoredPoint(id=1259714, version=0, score=0.8696132, payload={}, vector=None),\n",
       " ScoredPoint(id=278015, version=0, score=0.86873066, payload={}, vector=None)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "v = embed(tokenizer, model, [\"je suis ravie et vous remercie, bravo !\"], batch_size=1)[0]\n",
    "#v = embed(tokenizer, model, [documents[0][\"description\"]], batch_size=1)[0]\n",
    "\n",
    "\n",
    "res = client.search(collection_name=\"test\", query_vector=v, limit=3)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GD75Ki9D5JA",
    "outputId": "030bc72a-d885-4261-c790-b92ec996096c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24659 Très bonne accueil, très gentils , de bons conseils merci beaucoup !\n",
      "18362 Rien à améliorer : c'est parfait comme ça, bravo !\n",
      "7483 Merci à la personne  qui m'a aidé elle était très attentive et très rapide. bravo et bonne continuation\n"
     ]
    }
   ],
   "source": [
    "for r in res:\n",
    "  for i, d in enumerate(documents):\n",
    "    if r.id == d[\"id_experience\"]:\n",
    "      print(i, d[\"description\"])\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bEHkR0oGqIi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
