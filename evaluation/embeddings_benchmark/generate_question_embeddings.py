
#A script to generate synthetic questions for embedding evaluation.
#Texts are generated by Mistral-Hermes: no dependency to chatGPT, much much quicker, slight quality hit.
#Questions should be filtered afterwards (mandatory "?" at the end + max length)

import pandas as pd
from vllm import LLM, SamplingParams
import os
import json
from pprint import pprint


def get_hermes_question(user_input, mode = "interview"):
  sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=500, presence_penalty = 2)
  detailed_prompt = """<|im_start|>system
Rédige une courte question en français dont ce texte contiendrait la réponse<|im_end|>
<|im_start|>user"""
  detailed_prompt = detailed_prompt + "\n" + user_input + "<|im_end|>\n<|im_start|>assistant\n"
  prompts = detailed_prompt
  return prompts

# Opening JSON file
f = open('sheets_as_chunks.json')

# returns JSON object as
# a dictionary
data = json.load(f)
texts = []
prompts = []
identifier_text = []
for instruction in data:
    title, introduction, text = instruction["title"], instruction["introduction"], instruction["text"]
    if "context" in instruction:
        context = " > ".join(instruction["context"])
        combined_text = " ".join([title, introduction, text, context])
    else:
        combined_text = " ".join([title, introduction, text])
    prompts.append(get_hermes_question(combined_text))
    texts.append(combined_text)
    identifier_text.append(instruction["hash"])

llm = LLM("mistral-hermes")
sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=500)

outputs = llm.generate(prompts, sampling_params)

generated_text = []
for output in outputs:
  output = output.outputs[0].text
  generated_text.append(output)

df = pd.DataFrame(list(zip(identifier_text, texts, generated_text)),
               columns =["id", "text", "question"])

# Save the DataFrame to a TSV file
df.to_csv('summary_questions.tsv', sep='\t', index=False)
